# Analyzing Usability Test Results
With usability testing complete, creating a [Usability Test Report](http://www.usability.gov/how-to-and-tools/methods/reporting-usability-test-results.html) is key in making the data digestible and thus actionable for stakeholders. Ideally, the data collected was already planned for and breaking them into distinct categories makes it easier to present the insights. The data can be divided into the following two categories and subcategories:
## Quantitative data
In a spreadsheet, capture these metrics for each task scenario for each participant.
-   Success rates
-   Task time
-   Error rates
-   Satisfaction ratings
-   Participant’s demographic data so data can be divided by more variables.
## Qualitative data
-   Observations about paths users took
-   Problems encountered. These should be simple and concise. Examples:
	-   Good: Clicked on link to ‘Registration’ instead of ‘Sign Up’.
	-   Poor: Clicked the wrong link.
	-   Poor: Had confusion about links.
-   Comments/recommendations made by the user
-   Responses to open-ended questions
## Local vs. global errors
In early stages of reviewing the data, usability problems will begin to become apparent, and by completion, the severity of the problem and its prevalence should become evident. “Local errors” are precisely that, users struggling with a task completion on a single page or location. “Global errors” are errors that may show up initially on a single page or location but if it spans multiple pages or functions. For example, users may not find a link within a page because of text density or visual affordance. Even though the issue was identified on a particular page, it may impact other pages where the text density or lack of affordance is the same. 
## Error severity
Severity ultimately is subjective and should be defined by the project or organization, but it’s equally important to rank the severity of the issue to aid in prioritization. In the same example, text density may be a global problem, but may ultimately be considered minor in impact, so it may not be given top priority. Assigning a severity rating to discovered issues is valuable to assess what to address first. 

A rating scale could be as simple as:
-   Critical:  If this is not fixed, users will not complete the task scenario.    
-   Serious:  Many users will be frustrated and may give up if this is not fixed.    
-   Minor:  Users are annoyed but not prevented from completing the scenario. These can be revisited later.
## Implement and Retest
As with all research, the goal is to improve the usability and design for the user. Implementing changes and making updates to issues discovered should prioritize the most severe usability issues. As the design process advocates, retesting or validating with digital tools helps ensure the solutions provided were effective and ultimately leads to the growth and development of a product.

# Quiz
1. T or F: It is standard practice to give stakeholders the raw data results and nothing else from a usability test. (F)
2. T or F: The severity rating for errors should always be the same in every usability study. (F, it is defined by the project or team)
3. T or F: "Users were upset about the bad visual design of the nav bar" is an example of a good qualitative insight. (F, it is not specific about what is meant by "bad design" or what errors were made)
4. Choose the correct answer: "Users reported an overall satisfaction score of 4.6 out of 5" 
	1. Quantitative data (correct)
	2. Qualitative data