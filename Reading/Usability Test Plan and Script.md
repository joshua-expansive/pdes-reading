# Usability Test Plan and Script
![](https://prodesigncurriculum.s3.us-east-2.amazonaws.com/pushing-buttons-2.png)

Once you have finalized the task scenarios, it's time to write a testing plan and script. The script is a discussion guide or protocol for the moderator to follow so that the tests are consistent and thorough.

Your test plans should include each of the sections below. The examples are from [Observing the User Experience: A Practitioner's Guide to User Research](https://www.sciencedirect.com/book/9780123848697/observing-the-user-experience).
## Test plan
Update your project research plan with any new details and information you've gathered since the project began: schedule changes, participant details, and an updated list of UX measures, goals, and targets.
## Script
### Introduction
The introduction provides the moderator and participant with helpful context. It also serves as an "ice-breaker" to make sure the participant is comfortable, and it gives them a chance to ask questions. There is usually no need to record this part of the session. (5-7 minutes)

*Example*
> Hi, welcome, thank you for coming. How are you? (Did you find the place okay? Any questions about the non-disclosure agreement? Etc.) I’m ______. I am helping  ______ understand how well one of their products works for the people who are its audience. This is  ______, who will be observing what we’re doing today. We’ve brought you here to see what you think of their product: what seems to work for you, what doesn’t, etc. This evaluation should take about an hour. We’re going to be video recording what happens here today, but the video is for analysis only. It’s primarily so I don’t have to sit here and scribble notes and I can concentrate on talking to you. It will be seen by some members of the development team, a couple of other people, and by me. It’s strictly for research and not for public broadcast, publicity, promotion, or laughing at holiday parties.
>
> Like I said, we’d like you to help us with a product we’re developing. It’s designed for people like you, so we’d really like to know what you think about it and what works and doesn’t work for you. It’s currently in an early stage of development, so not everything you’re going to see will work right.
>
> Now I’d like to read you what’s called a statement of informed consent. It’s a standard thing I read to everyone I interview. It sets out your rights as a person who is participating in this kind of research. As a participant in this research: You may stop at any time. You may ask questions at any time. You may leave at any time. There is no deception involved. Your answers are kept confidential. Any questions before we begin? Let’s start!

### Preliminary interview
The preliminary interview gathers valuable data for understanding the participant's comments later on. Ask general questions at the beginning to ease the participant into the session and build their confidence. Then move to more specific questions around the context of the product you are testing. Let the participant know that you will begin recording at the beginning of the preliminary interview. (5-15 minutes)    

*Example*
> - How much time do you normally spend on the web in a given week? 
> - How much of that is for work use, and how much of that is for personal use? 
> - Other than email, is there any one thing you do the most online? Do you ever shop online? What kinds of things have you bought? How often do you buy stuff online? 
> - Do you ever do research online for things that you end up buying in stores? Are there any categories of items that this happens with more often than others? Why? 
> - Is there anything you would never buy online? Why?

### Evaluation instructions
Getting your participant to say what they are thinking is a critical part of the evaluation. Even though you introduce the technique during the scripted instructions, you will likely need to emphasize it during the testing too. (2-3 minutes)

*Example*
> In a minute, I’ll ask you to turn on the monitor and we’ll take a look at the product, but let me give you some instructions about how to approach it.
>  
> The most important thing to remember when you’re using it is that you are testing the product, the product is not testing you. There is absolutely nothing that you can do wrong. Period. If anything seems broken or wrong or weird or especially, confusing, it’s not your fault. However, we’d like to know about it. So please tell us whenever anything isn’t working for you. 
> 
> Likewise, tell us if you like something. Even if it’s a feature, a color, or the way something is laid out, we’d like to hear about it. 
> 
> Be as candid as possible. If you think something is awful, please say so. Don’t be shy; you won’t hurt anyone’s feelings. Since it’s designed for people like you, we really want to know exactly what you think and what works and doesn’t work for you. 
> 
> Also, while you’re using the product I’d like you to say your thoughts aloud. That gives us an idea of what you’re thinking when you’re doing something. Just narrate what you’re doing, sort of as a play-by-play, telling me what you’re doing and why you’re doing it.
> 
> Are there any questions?

### First impressions
Testing for first impressions is optional but is almost always very helpful for understanding what affects focus and attention. Make sure you have not displayed the product to the participant until this step. (5-10 minutes)

> I’d like you to select “Forks” from the “Favorites” menu. \[Rapidly\] What’s the first thing your eyes are drawn to? What’s the next thing? 
> 
> What’s the first thought that comes into your mind when you see this page?
> 
> \[after 1–2 minutes\] What is this site about? Are you interested in it? If this was your first time here, what would you do next? What would you click on? What would you be interested in investigating?

### Tasks
The tasks and probing questions are the bulk of the evaluation time. If you are reading them aloud, you must read the tasks exactly as written to ensure they are the same for each participant. Otherwise, you may print out or email the list of tasks to the participant to read, but not until the beginning of the test. (20-30 minutes)

*Example*
> Now I’d like you to try a couple of things with this interface. Work just as you would normally, narrating your thoughts as you go along. Here is the list of things I’d like you to do. 
>
> \[Hand out list.\] 
>
> The first scenario goes as follows: Let’s go back to the first page you saw on this site. I know you’re planning to replace some forks from your silverware set. If you wanted to find one of those forks on this website, how would you do that? Go ahead and show me. 
> 
> \[Hand out Task 1 description sheet.\]
> \[Give participant five minutes to locate the forks of their choice.\] 
> 
> Great, thanks. The second thing I’d like you to do is 
> \[TASK 2 DESCRIPTION WOULD GO HERE\] 
> \[Read the second task; hand out Task 2 description sheet.\]

### Probing questions
Probing questions can be asked at any appropriate point in the test. Including examples in the script will help you get answers to all the most important questions. 

*Example*
> Do the names of navigation elements make sense? Do the interface elements function as the evaluator had expected? Are there any interface elements that don’t make sense? What draws the evaluators’ attention?
> 
> What are the most important elements in any given feature? Are there places where the evaluator would like additional information? What are their expectations for the behavior/content of any given element/screen?

### Wrap up
The wrap-up section is a time for participants to tell you what parts of the interface stood out to them, good and bad. By asking them to brainstorm ideas, you may gain some valuable insights. (10–15 minutes)

*Example*
>  How would you describe this product in a couple of sentences to someone with a level of computer and web experience similar to yours? 
>  
>  Is this an interesting service? Is this something that you would use? Is this something you would recommend? Why/Why not? 
>  
>  Can you summarize what we’ve been talking about by saying three good things and three bad things about the product? 
>  
>  Okay, now that we’ve seen some of what this can do, let’s talk in blue-sky terms here for a minute. Not thinking in practical terms at all, what kinds of things would you like a system like this to do that this one doesn’t? 
>  
>  Have you ever said, “I wish that some program would do X for me”? What was it?
>  
>  Do you have any final questions? Comments? Thank you. If you have any other thoughts or ideas on your way home or tomorrow, or even next week, please feel free to send email to...

## Preparing for the test
Before conducting the first usability test, make a checklist of all of your materials needed and processes accounted for. It may be helpful to conduct a pilot test with a teammate or non-user. Be sure there is enough time between the pilot test and first usability test to iterate and make any necessary changes. Ideally, a designer would have 24-48 hours in case any issues arise.
-   Test run the equipment. All hardware and software that will be used from prototyping to recording the session.
-   Practice by reading the test script out loud. This can help gauge how clear your scenarios and questions are to the user.

# Quiz
1. Choose the correct answer: The introduction in a test script: 
	2. Gives the participant helpful context and information.  
	3. Gives the participant space to ask questions. 
	4. Makes sure the user has informed consent before beginning the test. 
	5. All of the above (correct)
2. T or F: It's best not to read task scenarios as written in the script because it seems robotic. (F)
3. T or F: When testing for first impressions you should make sure the user has already seen the product before the test (F)